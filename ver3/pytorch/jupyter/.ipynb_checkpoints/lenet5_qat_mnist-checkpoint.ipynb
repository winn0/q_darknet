{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e314fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "94.9%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%]\tLoss:2.305681\n",
      "Train Epoch: 0 [64/60000 (0%]\tLoss:2.292966\n",
      "Train Epoch: 0 [128/60000 (0%]\tLoss:2.297817\n",
      "Train Epoch: 0 [192/60000 (0%]\tLoss:2.287002\n",
      "Train Epoch: 0 [256/60000 (0%]\tLoss:2.282195\n",
      "Train Epoch: 0 [320/60000 (1%]\tLoss:2.270678\n",
      "Train Epoch: 0 [384/60000 (1%]\tLoss:2.266629\n",
      "Train Epoch: 0 [448/60000 (1%]\tLoss:2.259923\n",
      "Train Epoch: 0 [1024/60000 (2%]\tLoss:2.232528\n",
      "Train Epoch: 0 [1088/60000 (2%]\tLoss:2.242370\n",
      "Train Epoch: 0 [1152/60000 (2%]\tLoss:2.237648\n",
      "Train Epoch: 0 [1216/60000 (2%]\tLoss:2.223601\n",
      "Train Epoch: 0 [1280/60000 (2%]\tLoss:2.223080\n",
      "Train Epoch: 0 [1344/60000 (2%]\tLoss:2.245080\n",
      "Train Epoch: 0 [1408/60000 (2%]\tLoss:2.200924\n",
      "Train Epoch: 0 [1472/60000 (2%]\tLoss:2.222158\n",
      "Train Epoch: 1 [0/60000 (0%]\tLoss:0.591488\n",
      "Train Epoch: 1 [64/60000 (0%]\tLoss:0.740253\n",
      "Train Epoch: 1 [128/60000 (0%]\tLoss:0.623737\n",
      "Train Epoch: 1 [192/60000 (0%]\tLoss:0.718943\n",
      "Train Epoch: 1 [256/60000 (0%]\tLoss:0.572270\n",
      "Train Epoch: 1 [320/60000 (1%]\tLoss:0.295477\n",
      "Train Epoch: 1 [384/60000 (1%]\tLoss:0.561156\n",
      "Train Epoch: 1 [448/60000 (1%]\tLoss:0.662812\n",
      "Train Epoch: 1 [1024/60000 (2%]\tLoss:0.674754\n",
      "Train Epoch: 1 [1088/60000 (2%]\tLoss:0.816378\n",
      "Train Epoch: 1 [1152/60000 (2%]\tLoss:0.513414\n",
      "Train Epoch: 1 [1216/60000 (2%]\tLoss:0.598966\n",
      "Train Epoch: 1 [1280/60000 (2%]\tLoss:0.472660\n",
      "Train Epoch: 1 [1344/60000 (2%]\tLoss:1.014059\n",
      "Train Epoch: 1 [1408/60000 (2%]\tLoss:0.498602\n",
      "Train Epoch: 1 [1472/60000 (2%]\tLoss:0.697882\n",
      "Train Epoch: 2 [0/60000 (0%]\tLoss:0.305388\n",
      "Train Epoch: 2 [64/60000 (0%]\tLoss:0.444906\n",
      "Train Epoch: 2 [128/60000 (0%]\tLoss:0.322881\n",
      "Train Epoch: 2 [192/60000 (0%]\tLoss:0.442328\n",
      "Train Epoch: 2 [256/60000 (0%]\tLoss:0.246995\n",
      "Train Epoch: 2 [320/60000 (1%]\tLoss:0.130338\n",
      "Train Epoch: 2 [384/60000 (1%]\tLoss:0.259957\n",
      "Train Epoch: 2 [448/60000 (1%]\tLoss:0.413509\n",
      "Train Epoch: 2 [1024/60000 (2%]\tLoss:0.447037\n",
      "Train Epoch: 2 [1088/60000 (2%]\tLoss:0.334140\n",
      "Train Epoch: 2 [1152/60000 (2%]\tLoss:0.237160\n",
      "Train Epoch: 2 [1216/60000 (2%]\tLoss:0.272912\n",
      "Train Epoch: 2 [1280/60000 (2%]\tLoss:0.121312\n",
      "Train Epoch: 2 [1344/60000 (2%]\tLoss:0.699388\n",
      "Train Epoch: 2 [1408/60000 (2%]\tLoss:0.186180\n",
      "Train Epoch: 2 [1472/60000 (2%]\tLoss:0.366356\n"
     ]
    }
   ],
   "source": [
    "#trainning lenet5 mnist and save model\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from module.lenet5_module import *\n",
    "model_dir = os.getcwd()+'/saved_models'\n",
    "\n",
    "model_filename='le5.pt'\n",
    "model_filepath=model_dir+'/'+model_filename\n",
    "transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                              transform = transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, download=True,\n",
    "                              transform = transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset2, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_fp32 = Net()\n",
    "\n",
    "\n",
    "model_fp32.train() # 아래에 진행될 Quantization Aware Training logic이 작동하기 위해서는 모델을 train 모드로 바꿔줘야 한다고 한다.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "#model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv1', 'relu']]) #layerfussion 제외\n",
    "#model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32)\n",
    "model_fp32_prepared = model_fp32_prepared.to(\"cuda\")\n",
    "optimizer = optim.SGD(model_fp32_prepared.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "##CUDA를 이용해서 학습한다.\n",
    "for epoch in range(3):\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      output = model_fp32_prepared(data)\n",
    "      loss = F.nll_loss(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if batch_idx & 1000 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%]\\tLoss:{:.6f}'.format(\n",
    "            epoch, batch_idx*len(data), len(train_loader.dataset),\n",
    "            100.*batch_idx / len(train_loader), loss.item()\n",
    "        ))\n",
    "\n",
    "\n",
    "save_model(model=model_fp32_prepared, model_dir=model_dir, model_filename=model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd735c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0723, Accuracy: 9783/10000 (98%)\n",
      "\n",
      "inference를 할 때 걸린 시간(secs): 7.8588645458221436\n"
     ]
    }
   ],
   "source": [
    "###########fusionx load model\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                              transform = transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, download=True,\n",
    "                              transform = transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_fp32 = Net()\n",
    "model_fp32.train() # 아래에 진행될 Quantization Aware Training logic이 작동하기 위해서는 모델을 train 모드로 바꿔줘야 한다고 한다.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "#model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv1', 'relu']])\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32)\n",
    "model_fp32_prepared = model_fp32_prepared.to(\"cuda\")\n",
    "model_fp32_prepared = load_model(model=model_fp32_prepared, model_filepath=model_filepath, device='cpu')\n",
    "\n",
    "model_fp32_prepared.eval()\n",
    "model_int8_unfused = torch.quantization.convert(model_fp32_prepared.to('cpu')) #quantized aware training을 floating point로 수행한 model을 quantized integer model로 바꿔준다.\n",
    "\n",
    "\n",
    "\n",
    "model_int8_unfused.eval()\n",
    "\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "start2 = time.time()   \n",
    "count =0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to('cpu'), target.to('cpu') #GPU는 integer형 연산을 지원하지 않으므로 추론 속도를 비교하기 위해서 모델과 data를 모두 cpu로 옮겨줬다.\n",
    "        output = model_int8_unfused(data)\n",
    "        input_data =data\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)\n",
    "))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "#print(\"test 이전까지 경과 시간(secs):\",start2-start)\n",
    "print(\"inference를 할 때 걸린 시간(secs):\",end-start2)\n",
    "#print(\"total time elapsed(secs):\", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17505bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
