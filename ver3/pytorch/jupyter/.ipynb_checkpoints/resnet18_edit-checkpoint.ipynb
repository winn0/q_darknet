{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c227a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win0\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n",
      "C:\\Users\\win0\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:1109: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedResNet18(\n",
       "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (model_fp32): ResNet(\n",
       "    (conv1): Sequential(\n",
       "      (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.08831730484962463, zero_point=0, padding=(3, 3))\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (conv2_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.031286418437957764, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07724595814943314, zero_point=68, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03761531412601471, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.1419648975133896, zero_point=75, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv3_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.044798344373703, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.1405363231897354, zero_point=65, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): QuantizedConvReLU2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.0489143542945385, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.099954754114151, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.493866503238678, zero_point=65, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv4_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.27433034777641296, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.6802272796630859, zero_point=58, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): QuantizedConvReLU2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.19941933453083038, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.4281874895095825, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.8978381156921387, zero_point=70, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (conv5_x): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.4506237804889679, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.9777650833129883, zero_point=73, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential(\n",
       "          (0): QuantizedConvReLU2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.43366220593452454, zero_point=0)\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (residual_function): Sequential(\n",
       "          (0): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.6040323972702026, zero_point=0, padding=(1, 1))\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "          (3): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.7456293702125549, zero_point=120, padding=(1, 1))\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (shortcut): Sequential()\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.32349300384521484, zero_point=56, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####training q_resnet18\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from module.layer_fusion_module import *\n",
    "from module.resnet_module import *\n",
    "from module.extract_weight_module import *\n",
    "model_dir = \"saved_models\"\n",
    "model_filename = \"q_resnet18_cifar10.pt\"\n",
    "fusioned_model_filename = \"q_fusioned_resnet18_cifar10.pt\"\n",
    "quantized_model_filename_jit = \"q_resnet18_quantized_cifar10.pt\"\n",
    "quantized_model_filename = \"q_resnet18_quantized_cifar10.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "quantized_model_filepath_jit = os.path.join(model_dir, quantized_model_filename_jit)\n",
    "\n",
    "num_classes = 10\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "model = create_model(num_classes=num_classes)\n",
    "model.to(cpu_device)\n",
    "\n",
    "quantized_model = QuantizedResNet18(model_fp32=model)\n",
    "\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "quantized_model.qconfig = quantization_config\n",
    "fuse_model(quantized_model)\n",
    "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
    "quantized_model.to(cpu_device)    \n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "quantized_model=load_model(model=quantized_model, model_filepath=quantized_model_filepath, device='cpu')\n",
    "quantized_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ae53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized_model.quant.scale\n",
      "tensor([0.0374])\n",
      "quantized_model.quant.zero_point\n",
      "quantized_model.model_fp32.conv1[0].scale\n",
      "tensor(0.0883)\n",
      "quantized_model.model_fp32.conv1[0].zero_point\n",
      "quantized_model.model_fp32.conv2_x[0].residual_function[0].scale\n",
      "tensor(0.0313)\n",
      "quantized_model.model_fp32.conv2_x[0].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv2_x[0].residual_function[3].scale\n",
      "tensor(0.0772)\n",
      "quantized_model.model_fp32.conv2_x[0].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv2_x[1].residual_function[0].scale\n",
      "tensor(0.0376)\n",
      "quantized_model.model_fp32.conv2_x[1].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv2_x[1].residual_function[3].scale\n",
      "tensor(0.1420)\n",
      "quantized_model.model_fp32.conv2_x[1].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv3_x[0].residual_function[0].scale\n",
      "tensor(0.0448)\n",
      "quantized_model.model_fp32.conv3_x[0].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv3_x[0].residual_function[3].scale\n",
      "tensor(0.1405)\n",
      "quantized_model.model_fp32.conv3_x[0].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv3_x[0].shortcut[0].scale\n",
      "tensor(0.0489)\n",
      "quantized_model.model_fp32.conv3_x[0].shortcut[0].zero_point\n",
      "quantized_model.model_fp32.conv3_x[1].residual_function[0].scale\n",
      "tensor(0.1000)\n",
      "quantized_model.model_fp32.conv3_x[1].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv3_x[1].residual_function[3].scale\n",
      "tensor(0.4939)\n",
      "quantized_model.model_fp32.conv3_x[1].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv4_x[0].residual_function[0].scale\n",
      "tensor(0.2743)\n",
      "quantized_model.model_fp32.conv4_x[0].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv4_x[0].residual_function[3].scale\n",
      "tensor(0.6802)\n",
      "quantized_model.model_fp32.conv4_x[0].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv4_x[0].shortcut[0].scale\n",
      "tensor(0.1994)\n",
      "quantized_model.model_fp32.conv4_x[0].shortcut[0].zero_point\n",
      "quantized_model.model_fp32.conv4_x[1].residual_function[0].scale\n",
      "tensor(0.4282)\n",
      "quantized_model.model_fp32.conv4_x[1].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv4_x[1].residual_function[3].scale\n",
      "tensor(0.8978)\n",
      "quantized_model.model_fp32.conv4_x[1].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv5_x[0].residual_function[0].scale\n",
      "tensor(0.4506)\n",
      "quantized_model.model_fp32.conv5_x[0].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv5_x[0].residual_function[3].scale\n",
      "tensor(0.9778)\n",
      "quantized_model.model_fp32.conv5_x[0].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.conv5_x[0].shortcut[0].scale\n",
      "tensor(0.4337)\n",
      "quantized_model.model_fp32.conv5_x[0].shortcut[0].zero_point\n",
      "quantized_model.model_fp32.conv5_x[1].residual_function[0].scale\n",
      "tensor(0.6040)\n",
      "quantized_model.model_fp32.conv5_x[1].residual_function[0].zero_point\n",
      "quantized_model.model_fp32.conv5_x[1].residual_function[3].scale\n",
      "tensor(0.7456)\n",
      "quantized_model.model_fp32.conv5_x[1].residual_function[3].zero_point\n",
      "quantized_model.model_fp32.fc.scale\n",
      "tensor(0.3235)\n",
      "quantized_model.model_fp32.fc.zero_point\n"
     ]
    }
   ],
   "source": [
    "###save zero_point\n",
    "extract_model=quantized_model\n",
    "original_zeropoint_list=list()\n",
    "for parameter_name in quantized_model.state_dict():\n",
    "    \n",
    "    if parameter_name[-5:] == 'scale':\n",
    "\n",
    "        eval_line='quantized_model.'\n",
    "        c_past =''\n",
    "        for c in parameter_name:\n",
    "            if c_past =='.' and str.isdigit(c):\n",
    "                eval_line=eval_line[:-1]+'['\n",
    "                eval_line+=c\n",
    "                eval_line+=']'\n",
    "            else:\n",
    "                eval_line+=c     \n",
    "            c_past = c\n",
    "        print(eval_line)\n",
    "        print(quantized_model.state_dict()[parameter_name])\n",
    "    elif  parameter_name[-10:] == 'zero_point':\n",
    "        eval_line='quantized_model.'\n",
    "        c_past =''\n",
    "        for c in parameter_name:\n",
    "            if c_past =='.' and str.isdigit(c):\n",
    "                eval_line=eval_line[:-1]+'['\n",
    "                eval_line+=c\n",
    "                eval_line+=']'\n",
    "            else:\n",
    "                eval_line+=c     \n",
    "            c_past = c\n",
    "        print(eval_line)\n",
    "        if( 'quant.' not in eval_line):\n",
    "            exec(\"original_zeropoint_list.append(\"+eval_line+\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be68c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###rcovery zero_point\n",
    "count = 0\n",
    "for parameter_name in quantized_model.state_dict():\n",
    "    \n",
    "    if parameter_name[-5:] == 'scale':\n",
    "\n",
    "        eval_line='quantized_model.'\n",
    "        c_past =''\n",
    "        for c in parameter_name:\n",
    "            if c_past =='.' and str.isdigit(c):\n",
    "                eval_line=eval_line[:-1]+'['\n",
    "                eval_line+=c\n",
    "                eval_line+=']'\n",
    "            else:\n",
    "                eval_line+=c     \n",
    "            c_past = c\n",
    "        print(eval_line)\n",
    "        print(quantized_model.state_dict()[parameter_name])\n",
    "    elif  parameter_name[-10:] == 'zero_point':\n",
    "        eval_line='quantized_model.'\n",
    "        c_past =''\n",
    "        for c in parameter_name:\n",
    "            if c_past =='.' and str.isdigit(c):\n",
    "                eval_line=eval_line[:-1]+'['\n",
    "                eval_line+=c\n",
    "                eval_line+=']'\n",
    "            else:\n",
    "                eval_line+=c     \n",
    "            c_past = c\n",
    "        print(eval_line)\n",
    "        if( 'quant.' not in eval_line):\n",
    "            exec(eval_line+\" = original_zeropoint_list[\"+str(count)+\"]\")\n",
    "            count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b03cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::empty.memory_format' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at aten\\src\\ATen\\RegisterCUDA.cpp:26493 [kernel]\nMeta: registered at aten\\src\\ATen\\RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten\\src\\ATen\\RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten\\src\\ATen\\RegisterSparseCPU.cpp:958 [kernel]\nSparseCUDA: registered at aten\\src\\ATen\\RegisterSparseCUDA.cpp:1060 [kernel]\nBackendSelect: registered at aten\\src\\ATen\\RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:22 [kernel]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradMLC: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:11423 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7640\\3196229372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#make zeropoint 0 except quant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint8_eval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcpu_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"INT8 evaluation accuracy: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint8_eval_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparameter_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquantized_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\daknet_int8op\\completed_version\\ver2\\pytorch\\module\\resnet_module.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader, device, criterion)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\daknet_int8op\\completed_version\\ver2\\pytorch\\module\\resnet_module.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;31m# point to quantized in the quantized model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_fp32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;31m# manually specify where tensors will be converted from quantized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;31m# to floating point in the quantized model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\daknet_int8op\\completed_version\\ver2\\pytorch\\module\\resnet_module.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\daknet_int8op\\completed_version\\ver2\\pytorch\\module\\resnet_module.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidual_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::empty.memory_format' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at aten\\src\\ATen\\RegisterCUDA.cpp:26493 [kernel]\nMeta: registered at aten\\src\\ATen\\RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten\\src\\ATen\\RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten\\src\\ATen\\RegisterSparseCPU.cpp:958 [kernel]\nSparseCUDA: registered at aten\\src\\ATen\\RegisterSparseCUDA.cpp:1060 [kernel]\nBackendSelect: registered at aten\\src\\ATen\\RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:22 [kernel]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradMLC: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:10483 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_2.cpp:11423 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "#make zeropoint 0 except quant\n",
    "train_loader, test_loader = prepare_dataloader(num_workers=0, train_batch_size=128, eval_batch_size=256)\n",
    "_, int8_eval_accuracy = evaluate_model(model=quantized_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))    \n",
    "for parameter_name in quantized_model.state_dict():\n",
    "    \n",
    "    if parameter_name[-5:] == 'scale':\n",
    "\n",
    "        eval_line='quantized_model.'\n",
    "        c_past =''\n",
    "        for c in parameter_name:\n",
    "            if c_past =='.' and str.isdigit(c):\n",
    "                eval_line=eval_line[:-1]+'['\n",
    "                eval_line+=c\n",
    "                eval_line+=']'\n",
    "            else:\n",
    "                eval_line+=c     \n",
    "            c_past = c\n",
    "        print(eval_line)\n",
    "        print(quantized_model.state_dict()[parameter_name])\n",
    "    elif  parameter_name[-10:] == 'zero_point':\n",
    "        eval_line='quantized_model.'\n",
    "        c_past =''\n",
    "        for c in parameter_name:\n",
    "            if c_past =='.' and str.isdigit(c):\n",
    "                eval_line=eval_line[:-1]+'['\n",
    "                eval_line+=c\n",
    "                eval_line+=']'\n",
    "            else:\n",
    "                eval_line+=c     \n",
    "            c_past = c\n",
    "        print(eval_line)\n",
    "        if( 'quant.' not in eval_line):\n",
    "            exec(eval_line+\"=0\")\n",
    "            _, int8_eval_accuracy = evaluate_model(model=quantized_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "            print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_scale_list=list()\n",
    "\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv1[0].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv2_x[0].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv2_x[0].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv2_x[1].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv2_x[1].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv3_x[0].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv3_x[0].shortcut[0].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv3_x[0].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv3_x[1].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv3_x[1].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv4_x[0].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv4_x[0].shortcut[0].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv4_x[0].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv4_x[1].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv4_x[1].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv5_x[0].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv5_x[0].shortcut[0].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv5_x[0].q_func.scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv5_x[1].residual_function[3].scale\")\n",
    "connected_scale_list.append(\"quantized_model.model_fp32.conv5_x[1].q_func.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit shortcut connected scale\n",
    "for i in range(1,11):    \n",
    "    unified_scale_value=i*0.01\n",
    "    print(\"unified_scale_value:\",unified_scale_value)\n",
    "    for exec_line in connected_scale_list:\n",
    "        exec(exec_line+\"=\"+str(unified_scale_value))\n",
    "        #print(exec_line)\n",
    "    _, int8_eval_accuracy = evaluate_model(model=loaded_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
